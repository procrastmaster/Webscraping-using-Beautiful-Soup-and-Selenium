{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "* Using Beautiful Soup and Helium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import requests \n",
    "from helium import *\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading a table from a website\n",
    "# df = pd.read_html('https://quotes.toscrape.com/js/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the webpage content using requests\n",
    "# req = requests.get(url).content\n",
    "# soup_requests = BeautifulSoup(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the webpage content using helium\n",
    "url = 'https://quotes.toscrape.com/js/'\n",
    "browser = start_chrome(url, headless=True)\n",
    "webpage = BeautifulSoup(browser.page_source, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
      "“It is our choices, Harry, that show what we truly are, far more than our abilities.”\n",
      "“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
      "“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
      "“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\n",
      "“Try not to become a man of success. Rather become a man of value.”\n",
      "“It is better to be hated for what you are than to be loved for what you are not.”\n",
      "“I have not failed. I've just found 10,000 ways that won't work.”\n",
      "“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\n",
      "“A day without sunshine is like, you know, night.”\n"
     ]
    }
   ],
   "source": [
    "quotes = webpage.find_all('span', class_ = 'text')\n",
    "for quote in quotes:\n",
    "    print(quote.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>\n",
      "<a href=\"/\" style=\"text-decoration: none\">Quotes to Scrape</a>\n",
      "</h1>\n",
      "[<h1>\n",
      "<a href=\"/\" style=\"text-decoration: none\">Quotes to Scrape</a>\n",
      "</h1>]\n",
      "[<h1>\n",
      "<a href=\"/\" style=\"text-decoration: none\">Quotes to Scrape</a>\n",
      "</h1>]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Quotes to Scrape\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Login\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Next →\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<a href=\"/\" style=\"text-decoration: none\">Quotes to Scrape</a>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# navigation\n",
    "print(soup.body.div.h1)\n",
    "\n",
    "# find/find_all calls\n",
    "headers = soup.find_all(['h1','h2'])\n",
    "print(headers)\n",
    "\n",
    "# Nested find/find_all calls\n",
    "nest = soup.find('body').find('div').find_all('h1')\n",
    "print(nest)\n",
    "\n",
    "# get text values from an element\n",
    "text = soup.find('div')\n",
    "print(text.get_text())\n",
    "\n",
    "# Get a specific property from an element\n",
    "link = soup.find(\"a\")\n",
    "print(link['href'])\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading an image from a html website\n",
    "url_img = 'https://keithgalli.github.io/web-scraping/'\n",
    "req = requests.get(url_img+'webpage.html').content\n",
    "webpage_pic = BeautifulSoup(req)\n",
    "\n",
    "a = webpage_pic.find_all('div', class_ = 'column')\n",
    "img = a[2].find('img')['src']\n",
    "img_url = url + img\n",
    "\n",
    "img_data = requests.get(img_url).content\n",
    "with open ('img.jpg', 'wb') as downloader:\n",
    "    downloader.write(img_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "* Using Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import wget\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from getpass import getpass\n",
    "\n",
    "# XPath: helps address tag, attribute in an element\n",
    "# / --> selects root node, // --> starts from current node and matches anywhere\n",
    "# add a dot before xpath to address all xpaths in a page like ('.xpath')\n",
    "# tag: input[], attribute: @name=\"username\"\n",
    "# driver.find_element_by_xpath('//input[@name=\"username\"]')\n",
    "# handle = card.find_element_by_xpath('.//span[contains(text(), \"@\")]')\n",
    "# select_input = driver_ig.find_element_by_xpath(\"//a[contains(@href, \"keyword[1:]\")]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webscraping Youtube video data\n",
    "\n",
    "url_yt = 'https://www.youtube.com/c/google/videos?view=0&sort=p&flow=grid'\n",
    "\n",
    "driver_yt = webdriver.Chrome('C:\\\\Users\\\\Zuber\\\\chromedriver_win32\\\\chromedriver.exe')\n",
    "driver_yt.get(url)\n",
    "\n",
    "videos = driver_yt.find_elements_by_xpath('//*[@id=\"dismissible\"]')\n",
    "\n",
    "list_yt = []\n",
    "# add a dot before xpath to address all xpaths in a page like ('.xpath')\n",
    "for video in videos:\n",
    "    title = video.find_element_by_xpath('.//*[@id=\"video-title\"]').text\n",
    "    views = video.find_element_by_xpath('.//*[@id=\"metadata-line\"]/span[1]').text\n",
    "    time = video.find_element_by_xpath('.//*[@id=\"metadata-line\"]/span[2]').text\n",
    "    \n",
    "    item_yt = {\n",
    "        'title' : title,\n",
    "        'views' : views,\n",
    "        'time' : time\n",
    "    }\n",
    "\n",
    "    list_yt.append(item_yt)\n",
    "    \n",
    "df = pd.DataFrame(list_yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter password········\n"
     ]
    }
   ],
   "source": [
    "# Webscraping Instagram\n",
    "\n",
    "user = 'procrastmaster'\n",
    "pwd = getpass('Enter password')\n",
    "wait_time = 5\n",
    "\n",
    "url_ig = 'http://www.instagram.com'\n",
    "keyword = '@beeple_crap'\n",
    "\n",
    "driver_ig = webdriver.Chrome('C:\\\\Users\\\\Zuber\\\\chromedriver_win32\\\\chromedriver.exe')\n",
    "driver_ig.get(url_ig)\n",
    "time.sleep(wait_time)\n",
    "\n",
    "username = driver_ig.find_element_by_xpath('//input[@name=\"username\"]')\n",
    "username.send_keys(user)\n",
    "\n",
    "password = driver_ig.find_element_by_xpath('//input[@name=\"password\"]')\n",
    "password.send_keys(pwd)\n",
    "\n",
    "button1 = driver_ig.find_element_by_xpath('//button[@type=\"submit\"]')\n",
    "button1.click()\n",
    "time.sleep(wait_time)\n",
    "\n",
    "button2 = driver_ig.find_element_by_xpath('//button[contains(text(), \"Not Now\")]')\n",
    "button2.click()\n",
    "time.sleep(wait_time)\n",
    "\n",
    "button3 = driver_ig.find_element_by_xpath('//button[contains(text(), \"Not Now\")]')\n",
    "button3.click()\n",
    "time.sleep(wait_time)\n",
    "\n",
    "search_input = driver_ig.find_element_by_xpath('//input[@placeholder=\"Search\"]')\n",
    "search_input.send_keys(keyword)\n",
    "time.sleep(wait_time)\n",
    "\n",
    "select_input = driver_ig.find_element_by_xpath(\"//a[contains(@href, '/\" + keyword[1:] + \"/')]\")\n",
    "select_input.click()\n",
    "time.sleep(wait_time)\n",
    "\n",
    "n_scrolls = 2\n",
    "for j in range(0, n_scrolls):\n",
    "    driver_ig.execute_script(\"window.scrollTo(0, 5000);\")\n",
    "    time.sleep(wait_time)\n",
    "\n",
    "images_selenium = driver_ig.find_elements_by_tag_name('img')\n",
    "links = [ image.get_attribute('src') for image in images_selenium]\n",
    "\n",
    "time.sleep(wait_time)\n",
    "\n",
    "path = os.getcwd()\n",
    "path = os.path.join(path, keyword[1:])\n",
    "#create the directory\n",
    "os.mkdir(path)\n",
    "\n",
    "\n",
    "#download images\n",
    "counter = 0\n",
    "for image in links:\n",
    "    save_as = os.path.join(path, keyword[1:] + str(counter) + '.jpg')\n",
    "    wget.download(image, save_as)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
